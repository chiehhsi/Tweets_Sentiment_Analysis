{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chiehhsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chiehhsi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove unwanted patterns\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet Cleaner\n",
    "stopwordlist = set(stopwords.words(\"english\"))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    new_tweet = [w for w in tweet.split() if not w in stopwordlist] \n",
    "    return (\" \").join(new_tweet) \n",
    "\n",
    "def remove_specialchar(tweet):\n",
    "    #Converts HTML tags to the characters they represent\n",
    "    soup = BeautifulSoup(tweet, \"html.parser\")\n",
    "    tweet = soup.get_text()\n",
    "    \n",
    "    #Convert www.* or https?://* to empty strings\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) \n",
    "    #Convert @username to empty strings\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub('[\\n]+', ' ', tweet)\n",
    "    \n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    \n",
    "    #Trims the tweet\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    \n",
    "    #Remove all char not alphabets, num or whitespaces\n",
    "    tweet = re.sub('[^A-Za-z0-9 ]+','', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "#Attempts to replace every individual word with it's root word.\n",
    "def stemmer_lemmatizer(tweet):\n",
    "    word_list = []\n",
    "    for word in tweet.split():\n",
    "        word = ps.stem(word)\n",
    "        word = wordnet_lemmatizer.lemmatize(word)\n",
    "        word_list.append(word)\n",
    "    return (\" \".join(word_list))\n",
    "\n",
    "#for testing\n",
    "#Tweets = df_oba['Tweet'].values\n",
    "#for i in range(len(Tweets)):\n",
    "#    print(i)\n",
    "#    b = remove_specialchar(Tweets[i])\n",
    "#    a = stemmer_lemmatizer(b)\n",
    "#    print('last', a)\n",
    "#    print('last', remove_stopwords(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    xls = pd.ExcelFile('training-Obama-Romney-tweets.xlsx')\n",
    "    df1 = pd.read_excel(xls, 'Obama')\n",
    "    df2 = pd.read_excel(xls, 'Romney')\n",
    "    return df1, df2\n",
    "\n",
    "def data_parse(df):\n",
    "    \n",
    "    #drop missing values and rename columns\n",
    "    df = df.iloc[1:, 3:5]\n",
    "    df = df.rename(columns = {'Anootated tweet':'Tweet', 'Unnamed: 4': 'Class'})\n",
    "    # Drop rows if Tweet is empty\n",
    "    df.dropna(subset = ['Tweet'], inplace=True)\n",
    "    print(df.shape)\n",
    "    \n",
    "    #dtype class label\n",
    "    df['Class'] = df['Class'].astype(str)\n",
    "    df['Tweet'] = df['Tweet'].astype(str)\n",
    "    print(df.info())\n",
    "    print(df['Class'].value_counts())\n",
    "\n",
    "    # Extract rows where class labels -1, 0, 1\n",
    "    df = df[ (df['Class'] == '0') |(df['Class'] == '-1') | (df['Class'] == '1') ]\n",
    "    print('After extracting:', df.shape)\n",
    "    \n",
    "    tweets = df['Tweet'].values\n",
    "    targets = df['Class'].values\n",
    "    \n",
    "    return tweets, targets\n",
    "\n",
    "def tweet_cleaning(tweets):\n",
    "    for i in range(len(tweets)):\n",
    "        #print(i)\n",
    "        #print('original:', tweets[i])\n",
    "        tmp = remove_specialchar(tweets[i])\n",
    "        tmp = stemmer_lemmatizer(tmp)\n",
    "        tweets[i] = remove_stopwords(tmp)\n",
    "        #print('final:', tweets[i])\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(tweets, targets, tw_name, tar_name):\n",
    "    print('Saving training dataset...')\n",
    "    \n",
    "    #Create Saving Files\n",
    "    if not os.path.exists('TrainData'):\n",
    "        os.makedirs('TrainData')\n",
    "        \n",
    "    np.save('TrainData/' + tw_name + '.npy', tweets)\n",
    "    np.save('TrainData/' + tar_name + '.npy', targets)\n",
    "\n",
    "    print('Saved parsed dataset')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    df1, df2 = load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7196, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7196 entries, 1 to 7198\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Tweet   7196 non-null   object\n",
      " 1   Class   7196 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 168.7+ KB\n",
      "None\n",
      "0             1977\n",
      "-1            1968\n",
      "1             1679\n",
      "2             1543\n",
      "irrevelant      23\n",
      "nan              5\n",
      "irrelevant       1\n",
      "Name: Class, dtype: int64\n",
      "After extracting: (5624, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5624,), (5624,))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_oba, targets_oba = data_parse(df1)\n",
    "tweets_oba.shape, targets_oba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(tweets_oba)\n",
    "tweets_oba = tweet_cleaning(tweets_oba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['kirkpatrick wore basebal cap embroid barack obama signatur hangdog look jason segel courier journal',\n",
       "       'obama debat cracker cracker tonight I tune teamobama',\n",
       "       'miss point Im afraid understand bigger pictur dont care obama elect',\n",
       "       ...,\n",
       "       'reason ann romney michel obama match last night michel obama ann romney show last nig',\n",
       "       'obama kenakan cincin syahadat sejak sma',\n",
       "       'bitch like obama3 bitch want food stamp lmao'], dtype=object)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_oba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training dataset...\n",
      "Saved parsed dataset\n"
     ]
    }
   ],
   "source": [
    "save_dataset(tweets_oba, targets_oba, 'tweets_oba', 'targets_oba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7200 entries, 1 to 7200\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Tweet   7200 non-null   object\n",
      " 1   Class   7200 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 168.8+ KB\n",
      "None\n",
      "-1      2893\n",
      "0       1680\n",
      "2       1351\n",
      "1       1075\n",
      "!!!!     169\n",
      "nan       29\n",
      "IR         3\n",
      "Name: Class, dtype: int64\n",
      "After extracting: (5648, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5648,), (5648,))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_rom, targets_rom = data_parse(df2)\n",
    "tweets_rom.shape, targets_rom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Insidious!<e>Mitt Romney</e>'s Bain Helped Philip Morris Get U.S. High Schoolers <a>Hooked On Cigarettes</a> http://t.co/nMKuFcUq via @HuffPostPol\"\n",
      " '.@WardBrenda @shortwave8669 @allanbourdius you mean like <e>romney </e><a>cheated in primary</a>?'\n",
      " \"<e>Mitt Romney</e> still doesn't <a>believe</a> that we <a>have a black president</a>.\"\n",
      " ...\n",
      " 'el 59 por ciento de las mujeres blancas casadas respaldan a <e>Romney</e>\"\"\"'\n",
      " '\"And they brought us a whole binder of women\"\"\"\" oh <e>Romney</e>\"\"\"'\n",
      " '@FoxNews <e>Romney</e>won\"\"\"']\n"
     ]
    }
   ],
   "source": [
    "print(tweets_rom)\n",
    "tweets_rom = tweet_cleaning(tweets_rom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['insidiousmitt romney bain help philip morri get US high schooler hook On cigarett via',\n",
       "       'mean like romney cheat primari',\n",
       "       'mitt romney still doesnt believ black presid', ...,\n",
       "       'el 59 por ciento de la mujer blanca casada respaldan romney',\n",
       "       'brought u whole binder woman oh romney', 'romneywon'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training dataset...\n",
      "Saved parsed dataset\n"
     ]
    }
   ],
   "source": [
    "save_dataset(tweets_rom, targets_rom, 'tweets_rom', 'targets_rom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
